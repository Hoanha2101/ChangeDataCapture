"""
Kafka CDC Consumer â†’ Incremental RAG Embedding Pipeline
Láº¯ng nghe thay Ä‘á»•i tá»« MySQL qua Kafka, tá»± Ä‘á»™ng update embeddings
"""

from confluent_kafka import Consumer
import json
import time
from incremental_rag_system import CDCRAGIntegration, DataVersionTracker

# =============================================================================
# KAFKA CDC CONSUMER FOR RAG
# =============================================================================

class KafkaCDCToRAGConsumer:
    """
    Láº¯ng nghe Kafka messages tá»« CDC events,
    tá»± Ä‘á»™ng update Incremental RAG
    """
    
    def __init__(self, topic: str = "mysql-server.rag_db.stores"):
        self.conf = {
            'bootstrap.servers': 'localhost:9092',
            'group.id': 'rag-consumer',
            'auto.offset.reset': 'earliest',
            'enable.auto.commit': True
        }
        
        self.topic = topic
        self.consumer = Consumer(self.conf)
        self.consumer.subscribe([topic])
        
        self.rag_integration = CDCRAGIntegration()
        self.tracker = DataVersionTracker()
        
        self.embedding_queue = []  # Queue for chunks to embed
    
    def start(self, max_messages: int = None):
        """Báº¯t Ä‘áº§u láº¯ng nghe Kafka"""
        
        print("=" * 80)
        print("CDC â†’ Incremental RAG Pipeline")
        print("=" * 80)
        print(f"Listening on topic: {self.topic}")
        print("=" * 80)
        
        message_count = 0
        
        try:
            while True:
                msg = self.consumer.poll(1.0)
                
                if msg is None:
                    continue
                
                if msg.error():
                    print(f"Consumer error: {msg.error()}")
                    continue
                
                message_count += 1
                
                # Parse CDC event
                try:
                    cdc_event = json.loads(msg.value())
                    payload = cdc_event.get('payload', {})
                    
                    # Process event through Incremental RAG
                    result = self.rag_integration.process_cdc_event(payload)
                    
                    # Display results
                    self._display_event(message_count, result, payload)
                    
                    # Queue chunks for embedding
                    if result['needs_reembedding']:
                        self.embedding_queue.extend(result['chunks_to_update'])
                    
                    # Display embedding queue status
                    self._display_embedding_queue()
                    
                except json.JSONDecodeError as e:
                    print(f"[Error] Invalid JSON: {e}")
                    continue
                
                if max_messages and message_count >= max_messages:
                    break
        
        except KeyboardInterrupt:
            print("\n[Info] Consumer stopped by user")
        finally:
            self.consumer.close()
    
    def _display_event(self, count: int, result: dict, payload: dict):
        """Hiá»ƒn thá»‹ thÃ´ng tin CDC event"""
        
        op_map = {
            'c': 'â• CREATE',
            'u': 'âœï¸ UPDATE',
            'd': 'ğŸ—‘ï¸ DELETE'
        }
        
        op = payload.get('op', 'unknown')
        table = result.get('table', 'unknown')
        
        print(f"\n[Message #{count}] {op_map.get(op, op)}")
        print(f"  Table: {table}")
        print(f"  Changes: {len(result.get('changes', []))}")
        
        if result['changes']:
            print("  Changed fields:")
            for change in result['changes']:
                field = change.get('field', 'unknown')
                old_val = change.get('old', 'N/A')
                new_val = change.get('new', 'N/A')
                print(f"    â€¢ {field}: '{old_val}' â†’ '{new_val}'")
        
        print(f"  Chunks to embed: {len(result.get('chunks_to_update', []))}")
        print(f"  Needs re-embedding: {'âœ… Yes' if result['needs_reembedding'] else 'âŒ No'}")
    
    def _display_embedding_queue(self):
        """Hiá»ƒn thá»‹ queue chá» embedding"""
        
        if self.embedding_queue:
            print(f"\n  â³ Embedding Queue: {len(self.embedding_queue)} chunks waiting")
            print(f"     (Typically embed in batches of 32)")
            
            # Simulate batch embedding
            if len(self.embedding_queue) >= 32:
                self._process_embedding_batch(32)
    
    def _process_embedding_batch(self, batch_size: int):
        """Xá»­ lÃ½ embedding cho má»™t batch"""
        
        chunks = self.embedding_queue[:batch_size]
        self.embedding_queue = self.embedding_queue[batch_size:]
        
        print(f"\n  ğŸš€ Processing embedding batch ({len(chunks)} chunks)...")
        print(f"     (In production: send to embedding API - OpenAI, Cohere, local model)")
        
        # TODO: Thay báº±ng real embedding API
        # embeddings = embedding_model.embed_documents([c['content'] for c in chunks])
        # vector_db.add_vectors(chunks, embeddings)
        
        print(f"     âœ… Embedded {len(chunks)} chunks")
    
    def get_unprocessed_changes_stats(self):
        """Láº¥y thá»‘ng kÃª thay Ä‘á»•i chÆ°a xá»­ lÃ½"""
        
        changes = self.tracker.get_unprocessed_changes()
        
        print("\n" + "=" * 80)
        print("UNPROCESSED CHANGES STATISTICS")
        print("=" * 80)
        
        if not changes:
            print("âœ… All changes have been processed!")
            return
        
        # Thá»‘ng kÃª theo loáº¡i
        change_types = {}
        for change in changes:
            change_type = change['change_type']
            change_types[change_type] = change_types.get(change_type, 0) + 1
        
        print(f"Total unprocessed: {len(changes)}")
        print("\nBreakdown by type:")
        for change_type, count in change_types.items():
            print(f"  â€¢ {change_type}: {count}")
        
        # Nguá»“n thay Ä‘á»•i
        sources = {}
        for change in changes:
            source = change['source']
            sources[source] = sources.get(source, 0) + 1
        
        print("\nBreakdown by source:")
        for source, count in sources.items():
            print(f"  â€¢ {source}: {count}")


# =============================================================================
# BATCH EMBEDDING PROCESSOR
# =============================================================================

class BatchEmbeddingProcessor:
    """
    Xá»­ lÃ½ embedding chunks theo batch
    TÃ­ch há»£p vá»›i embedding APIs
    """
    
    def __init__(self, batch_size: int = 32):
        self.batch_size = batch_size
        self.embedding_queue = []
    
    def add_chunks(self, chunks: list):
        """ThÃªm chunks vÃ o queue"""
        self.embedding_queue.extend(chunks)
        
        # Process khi Ä‘á»§ batch size
        if len(self.embedding_queue) >= self.batch_size:
            self.process_batch()
    
    def process_batch(self):
        """Xá»­ lÃ½ má»™t batch"""
        
        if not self.embedding_queue:
            return
        
        batch = self.embedding_queue[:self.batch_size]
        self.embedding_queue = self.embedding_queue[self.batch_size:]
        
        # TODO: Integrate vá»›i embedding API
        # VÃ­ dá»¥:
        # from openai import OpenAI
        # client = OpenAI()
        # embeddings = client.embeddings.create(
        #     model="text-embedding-3-small",
        #     input=[c['content'] for c in batch]
        # )
        
        print(f"[Batch Processing] Embedded {len(batch)} chunks")
        
        # Update vector database
        # vector_db.add_vectors(batch, embeddings.data)


# =============================================================================
# MAIN - START LISTENING
# =============================================================================

def main():
    """Main entry point"""
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  CDC â†’ INCREMENTAL RAG PIPELINE                            â•‘
â•‘                                                                            â•‘
â•‘  This consumer listens to Kafka CDC events and automatically updates       â•‘
â•‘  RAG embeddings for changed data ONLY (not the entire dataset)             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # Táº¡o consumer
    consumer = KafkaCDCToRAGConsumer(topic="mysql-server.rag_db.stores")
    
    # Báº¯t Ä‘áº§u láº¯ng nghe
    try:
        consumer.start()
    except Exception as e:
        print(f"Error: {e}")
    finally:
        # Hiá»ƒn thá»‹ stats
        consumer.get_unprocessed_changes_stats()


if __name__ == "__main__":
    main()
