# ğŸš€ Incremental RAG System - Giáº£i PhÃ¡p HoÃ n Chá»‰nh

## ğŸ¯ Váº¥n Äá»

Báº¡n cÃ³ data RAG (`store_info.txt`) luÃ´n update liÃªn tá»¥c, nhÆ°ng:
- âŒ Má»—i láº§n update nhá» â†’ pháº£i re-embedding toÃ n bá»™ dá»¯ liá»‡u
- âŒ Tá»‘n thá»i gian, tÃ i nguyÃªn, API costs
- âŒ KhÃ´ng scalable vá»›i millions of updates

**Giáº£i phÃ¡p:** DÃ¹ng **Incremental RAG + CDC** Ä‘á»ƒ chá»‰ xá»­ lÃ½ nhá»¯ng pháº§n thay Ä‘á»•i!

---

## âœ… Kiáº¿n TrÃºc Giáº£i PhÃ¡p

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   OLD APPROACH (Full Re-embedding)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Data Update â†’ Chunking (100%) â†’ Embedding (100%) â†’ Vector DB   â”‚
â”‚                                   âš ï¸ 10 minutes for 10MB data    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        NEW APPROACH (Incremental with CDC)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  MySQL          Debezium       Kafka              Vector DB       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ UPDATE  â”œâ”€â”€â†’ â”‚ Capture  â”œâ”€â”€â†’ â”‚ CDC Topic   â”œâ”€â”€â†’ â”‚ Partial  â”‚   â”‚
â”‚  â”‚ phone#  â”‚    â”‚ Changes  â”‚   â”‚ mysql-...  â”‚   â”‚ Update   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                       â†“                           â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚                              â”‚ RAG Consumer   â”‚                   â”‚
â”‚                              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                   â”‚
â”‚                              â”‚ Detect Changed â”‚                   â”‚
â”‚                              â”‚ Fields ONLY    â”‚                   â”‚
â”‚                              â”‚ â†“              â”‚                   â”‚
â”‚                              â”‚ Incremental    â”‚                   â”‚
â”‚                              â”‚ Chunking (2%)  â”‚                   â”‚
â”‚                              â”‚ â†“              â”‚                   â”‚
â”‚                              â”‚ Embedding (2%) â”‚                   â”‚
â”‚                              â”‚ â†“              â”‚                   â”‚
â”‚                              â”‚ Update DB      â”‚                   â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                   âœ… 5 seconds only!              â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ï¿½ SÆ¡ Ä‘á»“ cÆ¡ cháº¿ hoáº¡t Ä‘á»™ng CDC â†” RAG

DÆ°á»›i Ä‘Ã¢y lÃ  sÆ¡ Ä‘á»“ vÃ  mÃ´ táº£ chi tiáº¿t tá»«ng bÆ°á»›c tá»« khi cÃ³ dá»¯ liá»‡u ban Ä‘áº§u Ä‘áº¿n khi RAG Ä‘Æ°á»£c cáº­p nháº­t incremental khi cÃ³ thay Ä‘á»•i nhá».

```
Initial Data (store_info.txt)
        â”‚
        â–¼
 1) Versioning & Full Chunking (láº§n Ä‘áº§u)
     - Táº¡o hash cá»§a file, lÆ°u vÃ o `file_versions`
     - Chia toÃ n bá»™ ná»™i dung thÃ nh chunks â†’ lÆ°u `chunks` + content_hash
        â”‚
        â–¼
 2) Embedding & Indexing (láº§n Ä‘áº§u)
     - Embed toÃ n bá»™ chunks â†’ upsert vÃ o Vector DB
     - Má»—i chunk kÃ¨m metadata (chunk_id, source_file)
        â”‚
        â–¼
 3) Ongoing Changes â†’ MySQL (updates/inserts/deletes)
     - Debezium captures binlog â†’ publish CDC event to Kafka topic
        â”‚
        â–¼
 4) CDC Topic (Kafka)
     - Messages: { op, before, after, source }
        â”‚
        â–¼
 5) RAG Consumer (Kafka â†’ Incremental pipeline)
     - Poll message
     - Parse payload (skip tombstone / msg.value()==None)
     - Map `op` (c/u/d) â†’ Decide: create/update/delete chunks
        â”‚
        â–¼
 6) Diff Detection & Incremental Chunking
     - For JSON: compare `before` vs `after` â†’ detect changed fields
     - Create small chunks only for changed fields/lines
     - Log each change to `changelog` for audit/retry
        â”‚
        â–¼
 7) Embedding & Partial Update
     - Batch changed chunks (eg. 32) â†’ call embedding API
     - Upsert embeddings to Vector DB (partial update)
     - Update `chunks` metadata (content_hash, embedding)
        â”‚
        â–¼
 8) Post-processing
     - Mark changelog entries processed
     - Optionally: warm caches, update search indices

```

Ghi chÃº quan trá»ng:
- Tombstones (DELETE): Debezium cÃ³ thá»ƒ gá»­i message vá»›i `after = null` hoáº·c `msg.value() = null` â€” xá»­ lÃ½ báº±ng cÃ¡ch xÃ³a vector hoáº·c Ä‘Ã¡nh dáº¥u `is_deleted`.
- Idempotency: dÃ¹ng `chunk_id` vÃ  `content_hash` Ä‘á»ƒ trÃ¡nh embed trÃ¹ng láº·p.
- Ordering & Exactly-once: xá»­ lÃ½ theo offset Kafka ; replay cÃ³ thá»ƒ dá»±a trÃªn changelog + file_versions.
- Bulk updates: náº¿u thay Ä‘á»•i lá»›n (>50% content) â€” cÃ¢n nháº¯c `force_all=True` Ä‘á»ƒ re-chunk & re-embed toÃ n bá»™.

---

## ï¿½ğŸ“¦ Files Táº¡o ÄÆ°á»£c

```
rag/
â”œâ”€â”€ store_info.txt                      â† Your original data
â”œâ”€â”€ rag_metadata.db                     â† Version tracking (auto-created)
â”œâ”€â”€ incremental_rag_system.py           â† Core system (chiáº¿c trÃ¡i tim)
â”œâ”€â”€ kafka_cdc_to_rag.py                 â† CDC consumer for RAG
â””â”€â”€ README.md                           â† Documentation (file nÃ y)
```

---

## ğŸ”§ CÃ¡ch Hoáº¡t Äá»™ng

### **Step 1: Version Tracking** ğŸ“

```python
tracker = DataVersionTracker("rag/rag_metadata.db")

# Database lÆ°u:
# 1. file_versions: hash cá»§a file, láº§n cuá»‘i modify
# 2. chunks: má»—i chunk + embedding + hash
# 3. changelog: log táº¥t cáº£ thay Ä‘á»•i
```

**Lá»£i Ã­ch:**
- PhÃ¡t hiá»‡n ngay khi cÃ³ file update
- KhÃ´ng cáº§n so sÃ¡nh toÃ n bá»™ file
- CÃ³ changelog cho audit trail

### **Step 2: Diff Detection** ğŸ”

```python
old_text = "Phone: 0909 456 123"
new_text = "Phone: 0909 456 789"  # â† Thay Ä‘á»•i

changes = DataDiffDetector.detect_text_changes(old_text, new_text)
# Result: [{'type': 'modified', 'old': '0909 456 123', 'new': '0909 456 789'}]
```

**PhÃ¡t hiá»‡n Ä‘Æ°á»£c:**
- âœ… DÃ²ng thay Ä‘á»•i (modified)
- âœ… DÃ²ng thÃªm má»›i (added)
- âœ… DÃ²ng bá»‹ xÃ³a (deleted)

### **Step 3: Incremental Chunking** âœ‚ï¸

```python
# Láº§n Ä‘áº§u tiÃªn
chunks = chunker.create_chunks(text, "store_info", force_all=True)
# Result: 50 chunks (tá»« toÃ n bá»™ file)

# Láº§n tiáº¿p theo (khi cÃ³ update)
chunks = chunker.create_chunks(new_text, "store_info", force_all=False)
# Result: 2 chunks (chá»‰ tá»« nhá»¯ng pháº§n thay Ä‘á»•i)
# âœ… 96% Ã­t hÆ¡n!
```

### **Step 4: CDC Integration** ğŸ”„

Khi MySQL cÃ³ thay Ä‘á»•i â†’ Debezium capture â†’ Kafka event:

```json
{
  "op": "u",
  "before": { "id": 1, "phone": "0909 456 123" },
  "after":  { "id": 1, "phone": "0909 456 789" },
  "source": { "table": "stores", "db": "rag_db" }
}
```

Kafka consumer tá»± Ä‘á»™ng:
1. PhÃ¡t hiá»‡n thay Ä‘á»•i
2. Táº¡o chunks chá»‰ cho `phone` field
3. Queue lÃªn embedding
4. Update vector database

---

## ğŸš€ Quick Start

### **Setup**

```cmd
REM 1. Start CDC system
docker-compose up -d

REM 2. Create database & connector
create_connector.bat

REM 3. Run RAG consumer
python rag/kafka_cdc_to_rag.py
```

### **Make Changes & Watch Auto-Update**

```cmd
REM Thay Ä‘á»•i dá»¯ liá»‡u
docker-compose exec -T mysql mysql -uroot -proot rag_db -e ^
  "UPDATE stores SET phone='0909 777 888' WHERE id=1;"

REM Xem automatic update trong RAG consumer terminal
REM Output sáº½ hiá»‡n:
REM   [Message #1] âœï¸ UPDATE
REM   Changed fields:
REM     â€¢ phone: '0909 456 123' â†’ '0909 777 888'
REM   Chunks to embed: 1
REM   Needs re-embedding: âœ… Yes
```

---

## ğŸ’¡ Comparison: Old vs New

### **Old Approach** (Full Re-embedding)
```
Dá»¯ liá»‡u: 1000 chunks
Update: 1 chunk

Action:
1. âœ— Re-chunk toÃ n bá»™ 1000 chunks (1 phÃºt)
2. âœ— Re-embed 1000 chunks (5 phÃºt)
3. âœ— Re-index vector DB (2 phÃºt)

Total: ~8 phÃºt + API costs cho 1000 embeddings
```

### **New Approach** (Incremental)
```
Dá»¯ liá»‡u: 1000 chunks
Update: 1 chunk

Action:
1. âœ“ Chunk chá»‰ changed field (1 giÃ¢y)
2. âœ“ Embed chá»‰ 1 chunk (0.5 giÃ¢y)
3. âœ“ Update vector DB (1 giÃ¢y)

Total: ~3 giÃ¢y + API costs cho 1 embedding (99.9% tiáº¿t kiá»‡m!)
```

---

## ğŸ“Š Performance Metrics

| Metric | Old Approach | New Approach | Improvement |
|--------|-------------|------------|-------------|
| **Processing Time** | 10 min | 5 sec | **120x faster** |
| **API Calls** | 1000 | 1 | **1000x less** |
| **Storage I/O** | 1000 chunks | 1 chunk | **1000x less** |
| **Cost (embeddings)** | $1.00 | $0.001 | **1000x cheaper** |
| **Vector DB Updates** | Full reindex | Partial update | **Much faster** |
| **Real-time capability** | âŒ No | âœ… Yes | **Live updates** |

---

## ğŸ”Œ Integration Points

### **1. With Embedding APIs**

```python
# OpenAI Embeddings
from openai import OpenAI

client = OpenAI()
embeddings = client.embeddings.create(
    model="text-embedding-3-small",
    input=[c['content'] for c in changed_chunks]
)

# Update vector DB
vector_db.add_vectors(changed_chunks, embeddings.data)
```

### **2. With Vector Databases**

```python
# Pinecone
import pinecone

index = pinecone.Index("store-info")
index.upsert(vectors=[
    (chunk['chunk_id'], embedding, chunk)
    for chunk, embedding in zip(changed_chunks, embeddings)
])

# Or: Weaviate, Milvus, ChromaDB, etc.
```

### **3. With RAG Framework**

```python
# LangChain
from langchain.vectorstores import Pinecone
from langchain.embeddings import OpenAIEmbeddings

vectorstore = Pinecone.from_documents(
    documents=changed_chunks,
    embedding=OpenAIEmbeddings(),
    index_name="store-info"
)
```

---

## ğŸ“ Key Concepts

### **1. Version Tracking**
- Má»—i file/chunk cÃ³ hash
- PhÃ¡t hiá»‡n changes through hashing
- Metadata stored in SQLite

### **2. Differential Updates**
- So sÃ¡nh old vs new versions
- Chá»‰ process differences
- Reduce overhead dramatically

### **3. Changelog Management**
- Log táº¥t cáº£ changes
- Audit trail for compliance
- Replay capability

### **4. Batch Processing**
- Group chunks into batches (32 default)
- Efficient API usage
- Better throughput

---

## ğŸ› ï¸ Advanced Configuration

### **Adjust Chunk Size**

```python
chunker = IncrementalChunker(
    chunk_size=1000,    # Words per chunk
    overlap=100         # Overlap between chunks
)
```

### **Batch Size for Embeddings**

```python
processor = BatchEmbeddingProcessor(batch_size=64)  # Default 32
```

### **Custom Change Detection**

```python
# For JSON data
changes = DataDiffDetector.detect_json_changes(old_data, new_data)

# For text
changes = DataDiffDetector.detect_text_changes(old_text, new_text)
```

---

## ğŸ“ˆ Scaling Scenarios

### **Scenario 1: 1M products with daily updates**
```
Without Incremental: 1M embeddings/day = $100/day
With Incremental: 1K embeddings/day = $0.10/day
Savings: $36,500/year!
```

### **Scenario 2: Real-time price updates**
```
10,000 price updates/hour

Old approach:
- Queue updates â†’ Batch at end of day â†’ Re-embed all data
- Latency: 12+ hours

New approach:
- CDC captures change immediately â†’ Incremental update
- Latency: <5 seconds
```

### **Scenario 3: Multi-language content**
```
Update affects: EN, VI, ZH versions (3x data)

Old: Re-embed 3x full dataset
New: Incremental updates = minimal overhead, supports all languages
```

---

## âš ï¸ Important Notes

### **When to Use Full Re-embedding**

```python
# Rare cases where full re-embedding is needed:

1. Schema changes (new fields in database)
chunker.create_chunks(text, source, force_all=True)

2. Embedding model upgrade
# All chunks need new embeddings

3. Vector DB migration
# Full reload recommended
```

### **Handling Large Bulk Updates**

```python
# If bulk update > 50% of data:
# - Still use incremental (tracks all changes)
# - But consider batching them over time
# - Or use separate bulk loading job

if len(changes) / total_chunks > 0.5:
    print("Large bulk update detected")
    print("Consider batch processing")
```

---

## ğŸ”— Next Steps

1. **Deploy to Production**
   - Set up monitoring
   - Configure backups of rag_metadata.db
   - Set up alerts for failed embeddings

2. **Optimize Performance**
   - Profile your workload
   - Tune batch sizes
   - Use appropriate embedding model

3. **Monitor & Maintain**
   - Track changelog growth
   - Periodic metadata cleanup
   - Version your embeddings

---

## ğŸ“ Support & Troubleshooting

**Q: No changes detected?**
A: Check if CDC is capturing events to Kafka

**Q: Embedding queue growing?**
A: Increase batch size or embedding API throughput

**Q: High memory usage?**
A: Reduce chunk size or process batches more frequently

---

**Happy Incremental RAG-ing!** ğŸ‰
