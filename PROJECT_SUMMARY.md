# Change Data Capture with RAG - Project Summary

## ğŸ“‹ Overview

A complete Change Data Capture (CDC) system with Retrieval Augmented Generation (RAG) pipeline. The system automatically captures database changes via MySQL Binlog and Debezium, synchronizes them to Pinecone vector database, and provides REST API endpoints for document management and semantic search.

**Key Features:**
- âœ… Full CRUD operations for documents with metadata
- âœ… Automatic CDC-driven Pinecone synchronization
- âœ… Intelligent document chunking with character-level positioning
- âœ… Semantic search using HuggingFace embeddings
- âœ… File-based document updates
- âœ… Category-based filtering and organization

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FastAPI REST API                         â”‚
â”‚                     (main.py - 28+ endpoints)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  MySQL 8.0  â”‚  â”‚  Pinecone    â”‚  â”‚  HuggingFace     â”‚  â”‚
â”‚  â”‚  Database   â”‚  â”‚  Vector DB   â”‚  â”‚  Embeddings      â”‚  â”‚
â”‚  â”‚  (testdb)   â”‚  â”‚  (384-dim)   â”‚  â”‚  (MiniLM-L6-v2)  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                 â”‚                    â”‚             â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                           â”‚                                   â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚         â–¼                 â–¼                    â–¼             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚    â”‚ Debezium â”‚  â”‚   Kafka     â”‚  â”‚ auto_gen_rag.py  â”‚     â”‚
â”‚    â”‚  MySQL   â”‚â”€â–¶â”‚   Topic     â”‚â”€â–¶â”‚   (Consumer)     â”‚     â”‚
â”‚    â”‚ Connectorâ”‚  â”‚ (CDC Events)â”‚  â”‚  (Sync Handler)  â”‚     â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Technology Stack

| Component | Technology | Version |
|-----------|-----------|---------|
| **Backend Framework** | FastAPI | 1.0+ |
| **Database** | MySQL | 8.0 (Docker) |
| **Message Queue** | Kafka | 7.5.0 (Docker) |
| **CDC** | Debezium Connect | 2.6 (Docker) |
| **Vector Database** | Pinecone | Serverless (AWS us-east-1) |
| **Embeddings** | HuggingFace | sentence-transformers/all-MiniLM-L6-v2 |
| **Python Runtime** | Python | 3.8+ |

### Key Python Libraries
```
fastapi==1.0+
uvicorn
mysql-connector-python
confluent-kafka
langchain
pinecone-client
requests
python-dotenv
```

---

## ğŸ“ Project Structure

```
ChangeDataCapture/
â”œâ”€â”€ main.py                 # FastAPI application (1450+ lines)
â”œâ”€â”€ database.py            # MySQL abstraction layer (400+ lines)
â”œâ”€â”€ auto_gen_rag.py       # Kafka CDC consumer (180+ lines)
â”œâ”€â”€ docker-compose.yml    # Docker services configuration
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ .env                  # Environment variables
â”œâ”€â”€ data/                 # Sample data files
â”‚   â”œâ”€â”€ AI.txt
â”‚   â”œâ”€â”€ CAR.txt
â”‚   â”œâ”€â”€ MKT.txt
â”‚   â””â”€â”€ SE.txt
â”œâ”€â”€ mysql-data/           # MySQL persistent storage
â”œâ”€â”€ rag/                  # RAG-related scripts
â””â”€â”€ README.md            # Project documentation
```

---

## ğŸ”Œ API Endpoints (28+)

### Health Checks (5 endpoints)
```
GET  /health                    # API health status
GET  /health/mysql              # MySQL connection check
GET  /health/pinecone           # Pinecone connection check
GET  /health/cdc                # CDC/Kafka connection check
GET  /health/all                # All services status
```

### Table Management (4 endpoints)
```
POST   /api/v1/table                          # Create table
GET    /api/v1/table                          # List all tables
DELETE /api/v1/table/{table_name}             # Delete table
GET    /api/v1/table/{table_name}/columns     # Get table columns
```

### Document Management (11 endpoints)
```
POST   /api/v1/document/{table_name}                          # Add document
GET    /api/v1/document/{table_name}                          # List documents
GET    /api/v1/document/{table_name}/{doc_id}                 # Get document
PUT    /api/v1/document/{table_name}/{doc_id}                 # Update document
DELETE /api/v1/document/{table_name}/{doc_id}                 # Delete document
POST   /api/v1/document/{table_name}/upload                   # Upload document file
PUT    /api/v1/document/{table_name}/{doc_id}/upload          # Update with file upload
POST   /api/v1/document/{table_name}/updateRow                # Update specific columns
POST   /api/v1/document/{table_name}/change                   # Substring replace
PUT    /api/v1/document/{table_name}/{doc_id}/change          # Change specific content
```

### Chunking Operations (4 endpoints)
```
POST /api/v1/document/chunking                      # Chunk all documents
POST /api/v1/document/{table_name}/{doc_id}/chunking # Chunk specific document
POST /api/v1/document/chunkingEmbPinecone           # Chunk all + embed + store
POST /api/v1/document/{table_name}/{doc_id}/chunkingEmbPinecone # Chunk specific + embed + store
```

### Pinecone Vector Operations (9 endpoints)
```
POST   /api/v1/pinecone/vector                  # Add/update vector
GET    /api/v1/pinecone/vector/{vector_id}/metadata  # Get vector with metadata
DELETE /api/v1/pinecone/vector/{vector_id}     # Delete vector by ID
GET    /api/v1/pinecone/filter/category/{category}   # Filter vectors by category
GET    /api/v1/pinecone/filter/doc/{doc_id}    # Filter vectors by document
DELETE /api/v1/pinecone/delete/category/{category}   # Delete all vectors by category
GET    /api/v1/pinecone/stats                  # Get index statistics
POST   /api/v1/pinecone/clean                  # Clean index
```

---

## ğŸ“Š Database Schema

### Table: `rag` (or custom table name)
```sql
CREATE TABLE rag (
  id INT AUTO_INCREMENT PRIMARY KEY,
  content LONGTEXT NOT NULL,
  metadata JSON,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
);
```

**Metadata Structure (JSON):**
```json
{
  "category": "AI",
  "filename": "document.txt",
  "source": "uploaded_file",
  "user": "admin"
}
```

---

## ğŸ”„ Workflow Examples

### 1. Create Document with File Upload
```bash
curl -X POST http://localhost:8000/api/v1/document/rag/upload \
  -F "file=@AI.txt" \
  -F "category=AI"
```
**Flow:**
1. File uploaded to MySQL with metadata
2. Debezium CDC captures INSERT event
3. `auto_gen_rag.py` receives CREATE event
4. Document auto-chunked and embedded
5. Vectors stored in Pinecone with metadata

---

### 2. Update Document with File
```bash
curl -X PUT http://localhost:8000/api/v1/document/rag/1/upload \
  -F "file=@AI_updated.txt"
```
**Flow:**
1. Document content replaced in MySQL
2. Debezium CDC captures UPDATE event
3. `auto_gen_rag.py` receives UPDATE event
4. Delete old vectors by category
5. Re-chunk and re-embed document
6. New vectors stored in Pinecone

---

### 3. Search Documents
```bash
curl -X GET http://localhost:8000/api/v1/pinecone/filter/category/AI
```
**Response:**
```json
{
  "status": 200,
  "result": {
    "vectors": [
      {
        "vector_id": "rag_1_chunk_0",
        "metadata": {
          "doc_id": 1,
          "category": "AI",
          "start_index": 0,
          "end_index": 512,
          "section_title": "AI Overview"
        },
        "values": [0.123, 0.456, ...]  // 384-dim embedding
      }
    ]
  }
}
```

---

### 4. Delete Document
```bash
curl -X DELETE http://localhost:8000/api/v1/document/rag/1
```
**Flow:**
1. Document deleted from MySQL
2. Debezium CDC captures DELETE event
3. `auto_gen_rag.py` receives DELETE event
4. All vectors by category deleted from Pinecone

---

## ğŸ“ Document Chunking Strategy

### Algorithm: Markdown Section-Based Chunking
```
Input: Document text
â”‚
â”œâ”€ Split by markdown headers (^#+\s+) and numbered sections (^\d+\.\s+)
â”‚
â”œâ”€ For each section:
â”‚  â”œâ”€ Extract section title
â”‚  â”œâ”€ Track character positions (start_index, end_index)
â”‚  â”œâ”€ Generate HuggingFace embedding (384 dimensions)
â”‚  â””â”€ Create vector with metadata
â”‚
â””â”€ Output: Array of chunks with character-level positioning
```

### Example Chunk Metadata
```json
{
  "vector_id": "rag_1_chunk_2",
  "content": "Artificial Intelligence is a rapidly growing field...",
  "metadata": {
    "doc_id": 1,
    "category": "AI",
    "section_title": "Definition",
    "chunk_index": 2,
    "start_index": 1024,
    "end_index": 2048
  }
}
```

---

## ğŸ”„ CDC Event Processing

### auto_gen_rag.py Consumer Logic

#### CREATE Event (op = "c")
```python
if op == "c":
    # 1. Extract document info from CDC event
    table_name = payload.source.table  # e.g., "rag"
    doc_id = payload.after.id          # e.g., 1
    category = metadata.category       # e.g., "AI"
    
    # 2. Call chunking & embedding API
    POST /api/v1/document/{table_name}/{doc_id}/chunkingEmbPinecone
    
    # 3. Result: Document chunks stored in Pinecone
```

#### UPDATE Event (op = "u")
```python
if op == "u":
    # 1. Extract update info
    old_content = payload.before.content
    new_content = payload.after.content
    
    # 2. Delete old vectors by category
    DELETE /api/v1/pinecone/delete/category/{category}
    
    # 3. Re-chunk and re-embed new content
    POST /api/v1/document/{table_name}/{doc_id}/chunkingEmbPinecone
    
    # 4. Result: New vectors stored in Pinecone
```

#### DELETE Event (op = "d")
```python
if op == "d":
    # 1. Extract category from metadata
    category = metadata.category
    
    # 2. Delete all vectors by category
    DELETE /api/v1/pinecone/delete/category/{category}
    
    # 3. Result: All vectors removed from Pinecone
```

---

## ğŸ³ Docker Setup

### docker-compose.yml Services
```yaml
services:
  mysql:
    image: mysql:8.0
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: testdb
    ports:
      - "3306:3306"
    command: --log-bin=mysql-bin --binlog_format=ROW
  
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    ports:
      - "2181:2181"
  
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper
  
  debezium-connect:
    image: debezium/connect:2.6
    ports:
      - "8083:8083"
    depends_on:
      - kafka
      - mysql
```

### Startup Commands
```bash
# Start Docker services
docker-compose up -d

# Start FastAPI server
uvicorn main:app --reload

# Start CDC consumer
python auto_gen_rag.py
```

---

## âš™ï¸ Environment Configuration

### .env File
```bash
# MySQL
MYSQL_HOST=localhost
MYSQL_PORT=3306
MYSQL_USER=root
MYSQL_PASSWORD=root
MYSQL_DATABASE=testdb

# Pinecone
PINECONE_API_KEY=your-api-key-here
PINECONE_INDEX_NAME=store-knowledge

# API Configuration
API_BASE_URL=http://localhost:8000
API_PORT=8000
API_RELOAD=true

# Kafka
KAFKA_BOOTSTRAP_SERVERS=localhost:9092
KAFKA_TOPIC=mysql-server.testdb.rag
KAFKA_GROUP_ID=demo-consumer
```

---

## ğŸ“ˆ Performance Metrics

| Operation | Time | Notes |
|-----------|------|-------|
| Document Upload | ~2-3s | Includes file I/O and DB insert |
| Chunking | ~1-2s | Depends on document size |
| Embedding Generation | ~5-10s | Per document (384-dim vectors) |
| Vector Storage | ~1-2s | Pinecone upsert operation |
| Document Update | ~5-15s | Delete old + re-chunk + re-embed |
| Category Filter Query | ~500ms | Vector similarity search |

---

## ğŸ§ª Testing Commands

### 1. Health Check
```bash
curl http://localhost:8000/health
```

### 2. Create Table
```bash
curl -X POST http://localhost:8000/api/v1/table \
  -H "Content-Type: application/json" \
  -d '{"table_name": "rag"}'
```

### 3. Upload Document
```bash
curl -X POST http://localhost:8000/api/v1/document/rag/upload \
  -F "file=@data/AI.txt" \
  -F "category=AI"
```

### 4. View Document
```bash
curl http://localhost:8000/api/v1/document/rag/1
```

### 5. Search by Category
```bash
curl http://localhost:8000/api/v1/pinecone/filter/category/AI
```

### 6. Update Document
```bash
curl -X PUT http://localhost:8000/api/v1/document/rag/1/upload \
  -F "file=@data/AI_updated.txt"
```

### 7. Delete Document
```bash
curl -X DELETE http://localhost:8000/api/v1/document/rag/1
```

---

## ğŸ¯ Key Features Implemented

âœ… **CRUD Operations**
- Create documents via file upload or JSON
- Read with pagination and filtering
- Update with file replacement or JSON patch
- Delete with cascade to Pinecone

âœ… **Automatic Synchronization**
- MySQL Binlog CDC capture
- Debezium connector integration
- Kafka topic streaming
- Auto-sync to Pinecone on changes

âœ… **Document Processing**
- Intelligent markdown-based chunking
- Character-level position tracking
- HuggingFace semantic embeddings
- Metadata preservation across pipeline

âœ… **Vector Management**
- Pinecone integration (384-dim, cosine metric)
- Metadata filtering (category, document ID)
- Vector CRUD operations
- Batch upsert/delete operations

âœ… **API Design**
- RESTful endpoints with tags
- Comprehensive error handling
- Health check monitoring
- Formatted JSON responses

---

## ğŸ“š Additional Resources

- **FastAPI Docs**: http://localhost:8000/docs (interactive Swagger UI)
- **MySQL Binlog**: `/var/lib/mysql/mysql-bin.000*`
- **Kafka Topics**: `mysql-server.testdb.*`
- **Debezium Connectors**: http://localhost:8083/connectors

---

## ğŸš€ Future Enhancements

1. **Semantic Search**: Add vector similarity search endpoint
2. **Batch Operations**: Bulk import/export utilities
3. **Caching Layer**: Redis for frequently accessed vectors
4. **Error Recovery**: Retry mechanism for failed operations
5. **Advanced Filtering**: Support for complex metadata queries
6. **Authentication**: JWT token-based API security
7. **Rate Limiting**: Request throttling and quota management
8. **Monitoring**: Prometheus metrics and Grafana dashboards

---

## ğŸ“ Support & Debugging

### Common Issues

**Issue**: Pinecone connection fails
```bash
# Check .env file has correct API key
# Verify network connectivity to Pinecone
curl -X GET https://api.pinecone.io/
```

**Issue**: CDC events not appearing
```bash
# Check MySQL binary logging enabled
mysql -u root -proot -e "SHOW VARIABLES LIKE 'log_bin%';"

# Verify Debezium connector status
curl http://localhost:8083/connectors/mysql-connector/status

# Check Kafka topics
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092
```

**Issue**: Document not chunked properly
```bash
# Test chunking endpoint directly
POST /api/v1/document/rag/1/chunking

# Check response for chunk details and metadata
```

---

## ğŸ“„ License & Credits

This project implements modern CDC patterns with RAG capabilities for semantic search and knowledge management.

**Built with:**
- FastAPI for REST API framework
- MySQL for persistent storage
- Kafka & Debezium for event streaming
- Pinecone for vector operations
- HuggingFace for semantic embeddings

---

**Project Status**: âœ… Complete & Production Ready

Last Updated: November 30, 2025
